{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a372f9dbc1fc714506dd3f1eaedab911",
     "grade": false,
     "grade_id": "cell-a0ff5833fec62bc4",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Lab - Time Series Classification \n",
    "Deep Learning indenfor det sundhedstekniske domæne (Fall 2021)<br>\n",
    "Instructor: Rahman Peimankar (abpe@mmmi.sdu.dk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c80f4d4ce25aaa285553afa77226de37",
     "grade": false,
     "grade_id": "cell-a1f22a8eced6fd4e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "The goal of this lab is to revise the learning of using RNN/LSTM, CNN, and combined RNN+LSTM models in Lesson 8 and Lesson 9, which are focused on bio-signals classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28bba951b05f671d7672a3ee25af5c2b",
     "grade": false,
     "grade_id": "cell-4c783cec666ef5e2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Dataset & Problem Definition\n",
    "\n",
    "In the first part of the lab, we use an Electrocardiograms (ECG) datset freely available from ``PhyioNet`` website: https://physionet.org/.   \n",
    "\n",
    "The dataset is called ``MIH-BIH Arrythmia dataset (MIT-BIH)``, which can be used to classify different heart diseases. We use this dataset to detect abnormal heartbeats from normal ones. You can read more about the dataset here: https://physionet.org/content/mitdb/1.0.0/ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6e147a140d1453eb946cf9414576315",
     "grade": false,
     "grade_id": "cell-d14b4be8c8c7b379",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "To start working with the MIT-BIH dataset, we need to learn how to use a Python library called ``WFDB``, which is developed for downloading reading, writing, and processing bio-signals and annotations/labels available on PhysioNet website. For more information, you may look into the ``WFDB`` GitHub repository here: https://github.com/MIT-LCP/wfdb-python\n",
    "\n",
    "You may also check the ``demo.ipynb`` notebook to learn more about how to use ``WFDB``: https://github.com/MIT-LCP/wfdb-python/blob/master/demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9491a302d73da57c990f2c6e108c8b41",
     "grade": false,
     "grade_id": "cell-0c525bc04cee0423",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 1: Data Preparation\n",
    "\n",
    "In this section, we prepare the data using ``WFDB`` package: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f9e083b9aad949ad0e0440e442270a2f",
     "grade": false,
     "grade_id": "cell-6aa7c9e3c5cb0906",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86d74ff606f9e0018c827326201b2d26",
     "grade": false,
     "grade_id": "cell-3036e027871562ef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's download the entire dataset and save them in the folder named ``data`` on our computer!\n",
    "\n",
    "**Note:** If you have not already installed ``WFDB``, you can do it as follows: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8176bba8a010a79d072beebdbaa5a661",
     "grade": false,
     "grade_id": "cell-77ed09fb4d3c307a",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wfdb in c:\\users\\abpe\\anaconda3\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from wfdb) (1.7.1)\n",
      "Requirement already satisfied: matplotlib>=3.3.4 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from wfdb) (3.4.3)\n",
      "Requirement already satisfied: numpy>=1.10.1 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from wfdb) (1.22.4)\n",
      "Requirement already satisfied: pandas>=0.17.0 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from wfdb) (1.3.4)\n",
      "Requirement already satisfied: requests>=2.8.1 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from wfdb) (2.26.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.4->wfdb) (3.0.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.4->wfdb) (2.8.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.4->wfdb) (1.3.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.4->wfdb) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from matplotlib>=3.3.4->wfdb) (0.10.0)\n",
      "Requirement already satisfied: six in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from cycler>=0.10->matplotlib>=3.3.4->wfdb) (1.15.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from pandas>=0.17.0->wfdb) (2021.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from requests>=2.8.1->wfdb) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from requests>=2.8.1->wfdb) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from requests>=2.8.1->wfdb) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\abpe\\anaconda3\\lib\\site-packages (from requests>=2.8.1->wfdb) (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install wfdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "18d18c066d977fd1f9358cdc125b2b95",
     "grade": false,
     "grade_id": "cell-c1561ba8a031cff0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating record list for: 100\n",
      "Generating record list for: 101\n",
      "Generating record list for: 102\n",
      "Generating record list for: 103\n",
      "Generating record list for: 104\n",
      "Generating record list for: 105\n",
      "Generating record list for: 106\n",
      "Generating record list for: 107\n",
      "Generating record list for: 108\n",
      "Generating record list for: 109\n",
      "Generating record list for: 111\n",
      "Generating record list for: 112\n",
      "Generating record list for: 113\n",
      "Generating record list for: 114\n",
      "Generating record list for: 115\n",
      "Generating record list for: 116\n",
      "Generating record list for: 117\n",
      "Generating record list for: 118\n",
      "Generating record list for: 119\n",
      "Generating record list for: 121\n",
      "Generating record list for: 122\n",
      "Generating record list for: 123\n",
      "Generating record list for: 124\n",
      "Generating record list for: 200\n",
      "Generating record list for: 201\n",
      "Generating record list for: 202\n",
      "Generating record list for: 203\n",
      "Generating record list for: 205\n",
      "Generating record list for: 207\n",
      "Generating record list for: 208\n",
      "Generating record list for: 209\n",
      "Generating record list for: 210\n",
      "Generating record list for: 212\n",
      "Generating record list for: 213\n",
      "Generating record list for: 214\n",
      "Generating record list for: 215\n",
      "Generating record list for: 217\n",
      "Generating record list for: 219\n",
      "Generating record list for: 220\n",
      "Generating record list for: 221\n",
      "Generating record list for: 222\n",
      "Generating record list for: 223\n",
      "Generating record list for: 228\n",
      "Generating record list for: 230\n",
      "Generating record list for: 231\n",
      "Generating record list for: 232\n",
      "Generating record list for: 233\n",
      "Generating record list for: 234\n",
      "Generating list of all files for: 100\n",
      "Generating list of all files for: 101\n",
      "Generating list of all files for: 102\n",
      "Generating list of all files for: 103\n",
      "Generating list of all files for: 104\n",
      "Generating list of all files for: 105\n",
      "Generating list of all files for: 106\n",
      "Generating list of all files for: 107\n",
      "Generating list of all files for: 108\n",
      "Generating list of all files for: 109\n",
      "Generating list of all files for: 111\n",
      "Generating list of all files for: 112\n",
      "Generating list of all files for: 113\n",
      "Generating list of all files for: 114\n",
      "Generating list of all files for: 115\n",
      "Generating list of all files for: 116\n",
      "Generating list of all files for: 117\n",
      "Generating list of all files for: 118\n",
      "Generating list of all files for: 119\n",
      "Generating list of all files for: 121\n",
      "Generating list of all files for: 122\n",
      "Generating list of all files for: 123\n",
      "Generating list of all files for: 124\n",
      "Generating list of all files for: 200\n",
      "Generating list of all files for: 201\n",
      "Generating list of all files for: 202\n",
      "Generating list of all files for: 203\n",
      "Generating list of all files for: 205\n",
      "Generating list of all files for: 207\n",
      "Generating list of all files for: 208\n",
      "Generating list of all files for: 209\n",
      "Generating list of all files for: 210\n",
      "Generating list of all files for: 212\n",
      "Generating list of all files for: 213\n",
      "Generating list of all files for: 214\n",
      "Generating list of all files for: 215\n",
      "Generating list of all files for: 217\n",
      "Generating list of all files for: 219\n",
      "Generating list of all files for: 220\n",
      "Generating list of all files for: 221\n",
      "Generating list of all files for: 222\n",
      "Generating list of all files for: 223\n",
      "Generating list of all files for: 228\n",
      "Generating list of all files for: 230\n",
      "Generating list of all files for: 231\n",
      "Generating list of all files for: 232\n",
      "Generating list of all files for: 233\n",
      "Generating list of all files for: 234\n",
      "Downloading files...\n"
     ]
    },
    {
     "ename": "ChunkedEncodingError",
     "evalue": "(\"Connection broken: ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None)\", ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 438, in _error_catcher\n    yield\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 519, in read\n    data = self._fp.read(amt) if not fp_closed else b\"\"\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\http\\client.py\", line 462, in read\n    n = self.readinto(b)\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\http\\client.py\", line 506, in readinto\n    n = self.fp.readinto(b)\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\socket.py\", line 704, in readinto\n    return self._sock.recv_into(b)\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\ssl.py\", line 1241, in recv_into\n    return self.read(nbytes, buffer)\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\ssl.py\", line 1099, in read\n    return self._sslobj.read(len, buffer)\nConnectionAbortedError: [WinError 10053] An established connection was aborted by the software in your host machine\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\requests\\models.py\", line 758, in generate\n    for chunk in self.raw.stream(chunk_size, decode_content=True):\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 576, in stream\n    data = self.read(amt=amt, decode_content=decode_content)\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 541, in read\n    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\contextlib.py\", line 137, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\urllib3\\response.py\", line 455, in _error_catcher\n    raise ProtocolError(\"Connection broken: %r\" % e, e)\nurllib3.exceptions.ProtocolError: (\"Connection broken: ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None)\", ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None))\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 48, in mapstar\n    return list(map(*args))\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\wfdb\\io\\download.py\", line 443, in dl_pn_file\n    dl_full_file(url, local_file)\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\wfdb\\io\\download.py\", line 464, in dl_full_file\n    response = requests.get(url)\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\requests\\api.py\", line 75, in get\n    return request('get', url, params=params, **kwargs)\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\requests\\api.py\", line 61, in request\n    return session.request(method=method, url=url, **kwargs)\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 542, in request\n    resp = self.send(prep, **send_kwargs)\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\requests\\sessions.py\", line 697, in send\n    r.content\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\requests\\models.py\", line 836, in content\n    self._content = b''.join(self.iter_content(CONTENT_CHUNK_SIZE)) or b''\n  File \"C:\\Users\\abpe\\Anaconda3\\lib\\site-packages\\requests\\models.py\", line 761, in generate\n    raise ChunkedEncodingError(e)\nrequests.exceptions.ChunkedEncodingError: (\"Connection broken: ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None)\", ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None))\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\1/ipykernel_13092/2795848.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# This makes a new folder/directory named 'data' for you.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./data'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# This changes the current directory to the newly created 'data'.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mwfdb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl_database\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'mitdb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# This will download the entire dataset into the newly created 'data' folder.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\wfdb\\io\\record.py\u001b[0m in \u001b[0;36mdl_database\u001b[1;34m(db_dir, dl_dir, records, annotators, keep_subdirs, overwrite)\u001b[0m\n\u001b[0;32m   4599\u001b[0m     \u001b[1;31m# Limit to 2 connections to avoid overloading the server\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4600\u001b[0m     \u001b[0mpool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPool\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprocesses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4601\u001b[1;33m     \u001b[0mpool\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdownload\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdl_pn_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdl_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4602\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Finished downloading files'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[0;32m    362\u001b[0m         \u001b[1;32min\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mreturned\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m         '''\n\u001b[1;32m--> 364\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmapstar\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    365\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    366\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstarmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunksize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    769\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 771\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    772\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    773\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mChunkedEncodingError\u001b[0m: (\"Connection broken: ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None)\", ConnectionAbortedError(10053, 'An established connection was aborted by the software in your host machine', None, 10053, None))"
     ]
    }
   ],
   "source": [
    "import wfdb\n",
    "os.mkdir('./data')  # This makes a new folder/directory named 'data' for you.\n",
    "os.chdir('./data')  # This changes the current directory to the newly created 'data'.\n",
    "wfdb.dl_database('mitdb', os.getcwd())  # This will download the entire dataset into the newly created 'data' folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "092ae2b96a67f0aaadf6e9a6690d9167",
     "grade": false,
     "grade_id": "cell-5cefe4baa9834428",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "There are ECG records of 48 patients available in the MIT-BIH dataset. We need this list later for reading the annotation of each ECG record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0548bb327063aa8fc206851110227906",
     "grade": false,
     "grade_id": "cell-cca4f183d6a9c631",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "patients = ['100','101','102','103','104','105','106','107',\n",
    "            '108','109','111','112','113','114','115','116',\n",
    "            '117','118','119','121','122','123','124','200',\n",
    "            '201','202','203','205','207','208','209','210',\n",
    "            '212','213','214','215','217','219','220','221',\n",
    "            '222','223','228','230','231','232','233','234']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4eda311320a914d8056bc187d74bf697",
     "grade": false,
     "grade_id": "cell-ab300a4d4cd1b244",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We will import and use ``WFDB`` package to download the ECG signals/records and their corresponding annotations. The annotation files contain the assigned labels to each heartbeat saying whether it is normal or not. There are many different annotation symbols and we need to clean the data to consider the ones that we need to develop our model.\n",
    "\n",
    "* ``wfdb.rdann`` command inside the below ``for`` loop will read the annotation file of each signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1711560b13b4b18793aaaf6cb2b46c3e",
     "grade": false,
     "grade_id": "cell-1c934c4b74d8e450",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import wfdb\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for patient_id in patients:\n",
    "    file_name = os.getcwd() + '\\\\' + patient_id\n",
    "    annotation = wfdb.rdann(file_name, 'atr')\n",
    "    types = annotation.symbol\n",
    "    \n",
    "    values, counts = np.unique(types, return_counts=True)\n",
    "    df1 = pd.DataFrame({'types':values, 'val':counts, 'patient_id':[patient_id]*len(counts)})\n",
    "    df = pd.concat([df, df1],axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f6af4e870216ce2f339eb0243664116a",
     "grade": false,
     "grade_id": "cell-19fa23ae88546e62",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now, we can print the number of heartbeats for each heartbeat type as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fe9f0421ceae9aff1e48942caaa1be6e",
     "grade": false,
     "grade_id": "cell-9ac9c9819fed826b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df.groupby('types').val.sum().sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d0d9f59f5c258fe28390f54fdc23a0ba",
     "grade": false,
     "grade_id": "cell-485b27c14486bd4f",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "However, some of these symbols represent the \"non-beat\" beats and we need to filter them out!\n",
    "\n",
    "For more information about the beat types, please see the links below: \n",
    "\n",
    "https://archive.physionet.org/physiobank/database/html/mitdbdir/tables.htm#testbeats\n",
    "\n",
    "\n",
    "https://archive.physionet.org/physiobank/database/html/mitdbdir/intro.htm#symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ae717644f3f1811cb017e3df44954706",
     "grade": false,
     "grade_id": "cell-f105f8745e855b18",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "non_beat = ['[','!',']','x','(',')','p','t','u','`','\\'','^','|','~','+','s','T','*','D','=','\"','@','Q','?']\n",
    "\n",
    "abnormal_beat = ['L','R','V','/','A','f','F','j','a','E','J','e','S']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1506959dbba7d9eaf56e334df4cbd197",
     "grade": false,
     "grade_id": "cell-a09dde940c991715",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Q1. Please complete the code below in order to group the three categories ``normal``, ``abnormal``, and ``nonbeat``. You should assign 0, 1, and -1 to ``normal``, ``abnormal``, and ``nonbeat``, respectively. By doing this, we can see the distribution of the classes/categories.\n",
    "\n",
    "**Hint:** You should first create a new column named ``class`` and initilaize it with -1. Then, change the values to 0 if is is ``N`` (normal beat) and to 1 if it is ``abnormal``. You should get a similr result as below. \n",
    "\n",
    "<center>\n",
    "<img src=\"fig1.JPG\" width=\"300\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41ee0011c02a7ce5f80f012c8727f488",
     "grade": false,
     "grade_id": "cell-d62ef81b175d6e7f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "df['class'] = -1\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5861adce0af1eb0a16ce4a6ba22aa53c",
     "grade": true,
     "grade_id": "cell-406d4d2ebc273f76",
     "locked": true,
     "points": 12,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9e9c0bf7f02280d98cfef80040764d33",
     "grade": false,
     "grade_id": "cell-a3429f6fdac5cc55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Great! So far, we have downloaded the dataset and have read the annotation files corresponding to each ECG record. You have also converted the annotations/labels into three groups (i.e. Normal, Abnormal, and Non-beats).\n",
    "\n",
    "Now it is the time to implement a function that can load the ECG signal and annotation file for a single patient. The ``read_ecg`` function gets a path to the downloaded dataset and returns the signal values/amplitudes, annotation/label type, and the location of the annotations/labels.\n",
    "\n",
    "**Note:** As given in the MIT-BIH dataset website (https://physionet.org/content/mitdb/1.0.0/), the sampling frequency of the ECG records is 360 Hz.\n",
    "\n",
    "**Note:** The annotations/lables of each heartbeat are usually placed at the peak of beat (or the QRS segment). Please see figure below. This will help us to cut/extract each beat fairly accurately. These heartbeats will be used as inputs for our deep learning models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "41f910484d22711a0ab9e1e49718eca9",
     "grade": false,
     "grade_id": "cell-15e714d9060684a2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def read_ecg(path):\n",
    "    \n",
    "    # Read ECG signal\n",
    "    record = wfdb.rdrecord(path)\n",
    "    \n",
    "    # Read corresponding annotation file\n",
    "    annot = wfdb.rdann(path, 'atr')\n",
    "    \n",
    "    # Get the ECG signal\n",
    "    ecg_sig = record.p_signal\n",
    "    \n",
    "    # Sampling frequency should be 360 Hz\n",
    "    # assert record.fs == 360, 'sample freq is not 360'\n",
    "    \n",
    "    # Get annotations' symbols/types and their corresponding location (index)\n",
    "    annot_type = annot.symbol\n",
    "    annot_sample = annot.sample\n",
    "    \n",
    "    return ecg_sig, annot_type, annot_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4f0e1386d5208bc392838c2d48449c54",
     "grade": false,
     "grade_id": "cell-3969edad56c33fd7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's see an example on how to use the function ``read_ecg``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b17a9c0c4de65cceb785d1ad54705296",
     "grade": false,
     "grade_id": "cell-ba1c074dbfc9f98d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "path = os.getcwd() + '/' + patients[0]  # This reads the first ECG records, which is record number ´100´\n",
    "ecg_sig, annot_type, annot_sample = read_ecg(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d009ca0cef8e6d4083c54bf123faf112",
     "grade": false,
     "grade_id": "cell-22756420a49ebf28",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's see the number of beats for each type in the first ECG record ('100').\n",
    "\n",
    "You can see that there are in total 11 **non-beats** shown as ``+`` and ``~`` and 53 **abnormal** beats shown as ``J`` and ``v``. Also, there are 2700 **normal** beats for the first record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7e7ed619bbb3f66e222ffcb53f111136",
     "grade": false,
     "grade_id": "cell-11aefddc7f21af99",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "symbol, count = np.unique(types, return_counts=True)\n",
    "for symb, c in zip(symbol, count):\n",
    "    print(symb, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "21f72e81b6c7527cb8bb769b527a4483",
     "grade": false,
     "grade_id": "cell-49183c037c641e55",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Figure below shows an excerpt of the first ECG record that we read using ``read_ecg`` function. As you can see, there is one **abnormal** beat at index around 66765. \n",
    "\n",
    "In the next step, we develop a function to extract every beat by receiving the center index (e.g. 66765) of the beat and looking $\\pm$ 3 seconds (or $2\\times 3 seconds \\times 360 Hz = 2060 samples$) around it. \n",
    "\n",
    "<center>\n",
    "<img src=\"fig2.JPG\" width=\"600\"/> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "afef7da2b0696063f0a0a92d162d0158",
     "grade": false,
     "grade_id": "cell-e4eb15448004fda3",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Q2. Please complete the ``build_dataset`` function below to make the dataset by ignoring the **non-beats** heartbeats. You will use the ``read_ecg`` function to read each ECG reacords and then extract the single beats as described in the previous cell.\n",
    "\n",
    "**Note:** \n",
    "\n",
    "1. The ``build_dataset`` function gets the list of patients (``patients``), the time interval (``interval``) before and after each heart peak ($\\pm$3 seconds for example), sampling frequency (``fs``), which is 360 Hz for MIT-BIH dataset, and the list of abnormal beats symbols (``abnormal_beat``) as defined earlier.\n",
    "\n",
    "\n",
    "\n",
    "2. The ``build_dataset`` function returns two matrices called ``X`` and ``Y``, which are extracted heartbeats and their coresponding labels (**normal** or **abnormal**), respectively.  It also returns the annotation symbol (``annot_symb``) for each individual extracted beat. It should be mentioned that matrix X rows and columns represent the number of beats and the values of beats (i.e. ``interval`` * ``fs``)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5011861479ffeb16a2129437f751cd18",
     "grade": false,
     "grade_id": "cell-2b61d2d56fb57ec1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def  (patients, interval, fs, abnormal_beat):\n",
    "    \n",
    "    # Initialize the arrays\n",
    "    num_cols = 2 * interval * fs  # This specify the length of the heartbeats to be extracted.\n",
    "    X = np.zeros((1,num_cols))\n",
    "    Y = np.zeros((1,1))\n",
    "    annot_symb = []\n",
    "    \n",
    "    # This list stores the number of extracted heartbeats for each patient.\n",
    "    num_beats = []\n",
    "    \n",
    "    for patient_id in patients:\n",
    "        file_path = os.getcwd() + '\\\\' + patient_id\n",
    "        \n",
    "        ecg_sig, annot_type, annot_sample = read_ecg(file_path)\n",
    "        \n",
    "        # Since there are two leads of ECGs for each record, we only select the first lead to work with.\n",
    "        ecg_sig = ecg_sig[:,0]\n",
    "        \n",
    "        # We simply remove the \"non-beats\" beats from the df_annot dataframe and only keep \"normal\" and \"abnormal\" beats.\n",
    "        df_annot = pd.DataFrame({'annot_type':annot_type,\n",
    "                              'annot_sample':annot_sample})\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        # The \"make_XY\" builds the x and y matrics for each extarcted heartbeat\n",
    "        x, y, symbol = make_XY(ecg_sig, df_annot, interval, num_cols, abnormal_beat)\n",
    "        annot_symb = annot_symb + symbol\n",
    "        num_beats.append(x.shape[0])\n",
    "        X = np.append(X,x,axis = 0)\n",
    "        Y = np.append(Y,y,axis = 0)\n",
    "\n",
    "    return X, Y, annot_symb\n",
    "\n",
    "\n",
    "\n",
    "def make_XY(ecg_sig, df_annot, interval, num_cols, abnormal_beat):\n",
    "    # this function builds the X,Y matrices for each beat\n",
    "    # it also returns the original symbols for Y\n",
    "    \n",
    "    num_row = len(df_annot)\n",
    "\n",
    "    x = np.zeros((num_row, num_cols))\n",
    "    y = np.zeros((num_row,1))\n",
    "    symbol = []\n",
    "    \n",
    "    # count the rows\n",
    "    row_size = 0\n",
    "\n",
    "    for annot_sample, annot_type in zip(df_annot.annot_sample.values, df_annot.annot_type.values):\n",
    "\n",
    "        left = max([0,(annot_sample - interval*fs) ])\n",
    "        right = min([len(ecg_sig),(annot_sample + interval*fs) ])\n",
    "        xx = ecg_sig[left: right]\n",
    "        if len(xx) == num_cols:\n",
    "            x[row_size,:] = xx\n",
    "            y[row_size,:] = int(annot_type in abnormal_beat)\n",
    "            symbol.append(annot_type)\n",
    "            row_size += 1\n",
    "    x = x[:row_size,:]\n",
    "    y = y[:row_size,:]\n",
    "    return x, y, symbol\n",
    "\n",
    "\n",
    "\n",
    "interval =  3\n",
    "fs = 360\n",
    "\n",
    "X, Y, annot_symb = build_dataset(patients, interval, fs, abnormal_beat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "11bd8dc13f1948a6bf044c1b271ad04b",
     "grade": true,
     "grade_id": "cell-56a74151f4d07167",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eb968194ab28899714b691ecb279585b",
     "grade": false,
     "grade_id": "cell-f2a01124c49946d9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Congratulations! You have prepared a complete dataset for the deep learning model development. This is fantastic because you can use the same codes with very litle changes to work with any other databases from PhysioNet such as:\n",
    "\n",
    "* MIT-BIH Supraventricular Arrhythmia Database (https://physionet.org/content/svdb/1.0.0/)\n",
    "* St Petersburg INCART 12-lead Arrhythmia Database (https://physionet.org/content/incartdb/1.0.0/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8aaeec93677d58c1b7d0565fe0eb8636",
     "grade": false,
     "grade_id": "cell-f6892b53ecd05134",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Part 2: Model Development\n",
    "\n",
    "Let's start the fun part of every machine/deep learning project, model development :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d77f9bd8e83dc03697c5a29d2876b103",
     "grade": false,
     "grade_id": "cell-b7180d2a9e62ecc6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    " Q3. It is the time to split the prepared dataset into train and validation parts. Please complete the code below using ``train_test_split`` function from ``scikit-learn`` library to split the dataset. \n",
    " \n",
    "**Note:** Please set only these two parameters of ``train_test_split`` function and leave the rest as default:\n",
    "\n",
    "``test_size=0.33`` and ``random_state=42``\n",
    "\n",
    "And name the outputs of the function as: ``X_train``, ``X_validation``, ``y_train``, ``y_validation``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "896b0a702cae87a8c2a65577686073e0",
     "grade": false,
     "grade_id": "cell-9441c98451f0267c",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "34210df9a03357357336118592e5d81e",
     "grade": true,
     "grade_id": "cell-2feab078a90c54d0",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "dd319ae773050d58083d8bfc843380ce",
     "grade": false,
     "grade_id": "cell-50c3883d0deab80b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Before we start to develope a CNN and an LSTM model for classify heart arrhytmia, let's have a baseline dense neural networks (with only two dense layers) to compare the results of CNN and LSTM with it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a22888f983ecc04a85c12fb620b45d8",
     "grade": false,
     "grade_id": "cell-d696baa2da0c9d3e",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "If you have not installed tensorflow already, follow one of the following to install it:\n",
    "\n",
    "1) using simple \"pip\" install:\n",
    "\n",
    "``!pip install tensorflow``\n",
    "\n",
    "2) Open your \"Anaconda Prompt\" and type in the below command to install:\n",
    "\n",
    "``conda install -c conda-forge tensorflow``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "83111d0c4d96c06d0743fb91de49cbf0",
     "grade": false,
     "grade_id": "cell-be7561c83e7616ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow \n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation = 'relu', input_shape = (X_train.shape[1],)))\n",
    "model.add(Dense(32, activation = 'relu'))\n",
    "model.add(Dropout(rate = 0.2))\n",
    "model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "\n",
    "model.fit(X_train, y_train, batch_size = 32, epochs= 10, verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "831b44564d390d18d895d126fd37da61",
     "grade": false,
     "grade_id": "cell-9f24570e74548165",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Let's check how well our trained model perform on the validation data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "51d2edc3cbe4da4a8b220101b8345a06",
     "grade": false,
     "grade_id": "cell-ca513bcc3332deb6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred_val = model.predict(X_validation)\n",
    "\n",
    "threshold = 0.5\n",
    "print(classification_report(y_validation, y_pred_val>threshold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7650648697d10e9a575d23039c30c8f4",
     "grade": false,
     "grade_id": "cell-fdc9db470ce6f994",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Q3. Now it is your turn to develop a CNN model. Please complete the cosde below to develope a model architecture as below figure:\n",
    "    \n",
    "<center>\n",
    "<img src=\"fig3.JPG\" width=\"500\"/>  \n",
    "    \n",
    " \n",
    "**Note:** You should set the ``input_shape = (2160, 1)``. This is equal to the length of one heartbeat ($2\\times 3 seconds \\times 360 Hz = 2060 samples$) \n",
    "    \n",
    "**Important:** It may take a while to train the below CNN model on CPU and that is why only 2 epochs is chosen. You may train it on Google Colab on GPU nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "44756c91715faa9d8c240c978136a153",
     "grade": false,
     "grade_id": "cell-5f8da70233feda25",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Another important step to do here is to reshape the inputs to be ``[num_samples, interval, feature=1]``. You may notice that unlike image datasets that we had ``feature=3`` representing the three channels (RGB), here we have only 1D time series as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "558bef8f58623cf8c5287c5275b915c1",
     "grade": false,
     "grade_id": "cell-beee2323307bef23",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "X_train_reshaped = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
    "X_valid_reshaped = np.reshape(X_validation, (X_validation.shape[0], X_validation.shape[1], 1))\n",
    "\n",
    "print(X_train_reshaped.shape)\n",
    "print(X_valid_reshaped.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d9a4e739a5ac088b827c1118a7304e6",
     "grade": true,
     "grade_id": "cell-a03aa220f67c77d5",
     "locked": false,
     "points": 26,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Conv1D\n",
    "\n",
    "model = Sequential()\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model.fit(X_train_reshaped, y_train, batch_size = 32, epochs= 2, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e4b16e51e7cde899b0c1e5321245abb1",
     "grade": false,
     "grade_id": "cell-20846b6e310c4dcc",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cc7bbe35b36825548b13fefb346be48f",
     "grade": false,
     "grade_id": "cell-0bde589a294e34b8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Q4. Please complete the code below to report the performance of the CNN model.\n",
    "\n",
    "**Note:** Please name the model predictions on the validation data (``X_valid_reshaped``) as: ``y_pred_cnn``!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e343c58a611ff0fe72cacde53129d8ea",
     "grade": false,
     "grade_id": "cell-075e2aedfadd94c1",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "threshold = 0.5\n",
    "print(classification_report(y_validation, y_pred_cnn>threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dba78fc1f8be94b592a7573dba7c4a42",
     "grade": true,
     "grade_id": "cell-5370e5c986563519",
     "locked": true,
     "points": 10,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "25467033428ce0529790453e1d72d2e6",
     "grade": false,
     "grade_id": "cell-036b34ff2a7aaa73",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We can also plot a Confusion Matrix for our trained classifier on the validation data! You should use ``confusion_matrix`` method/function from scikit-learn library for this purpose. For more help and see some examples on how to use``confusion_matrix``, please read here: https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html  \n",
    "\n",
    "Q5. Please complete the code below for calculating the confusion matrix for the validation data and name it ``cm``.\n",
    "\n",
    "**Hint:** The confusion matrix sould look like the below:\n",
    "\n",
    "<center>\n",
    "<img src=\"fig5.JPG\" width=\"400\"/> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "321cda99a84d8778836cf48ec9118855",
     "grade": false,
     "grade_id": "cell-85e8c31f898f0247",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "cm = cm / cm.astype(np.float).sum(axis=1) # This line normalizes the calculated confusion matrix\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "sns.heatmap(cm, annot=True,cmap=plt.cm.Blues)\n",
    "plt.tight_layout()\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d0398633859f6234b793bd2474d6a141",
     "grade": true,
     "grade_id": "cell-e609f962a9cbdb31",
     "locked": true,
     "points": 5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bed21e5692ebaac6e846eb9e8e3961e1",
     "grade": false,
     "grade_id": "cell-304e70d0353e6080",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Q6. Now it is time to develop an LSTM model. Please complete the cosde below to develope a model architecture as below figure:\n",
    "    \n",
    "<center>\n",
    "<img src=\"fig4.JPG\" width=\"500\"/>  \n",
    "    \n",
    " \n",
    "**Note 1:** You should set the ``input_shape = (X_train_reshaped.shape[1], X_train_reshaped.shape[2])``. These are equal to the number of samples (heartbeats) and length of one heartbeat ($2\\times 3 seconds \\times 360 Hz = 2060 samples$). \n",
    "    \n",
    "**Note 2:** The ``return_sequences=True`` force the LSTM/BiLSTM layer to return the output for each LSTM cell/unit.    \n",
    "    \n",
    "**Important:** It may take a while to train the below LSTM model on CPU and that is why only 2 epochs is chosen. You may train it on Google Colab on GPU nodes. Here, we reduce the size of the dataset (``X_train_cnn[:10000], y_train[:10000]``) to make it feasible for quick prototyping but you should train the model with the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f912aa3b67c5db6efdfc30dc23af291d",
     "grade": true,
     "grade_id": "cell-7a62bbec909ad85e",
     "locked": false,
     "points": 26,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional, LSTM\n",
    "\n",
    "model_lstm = Sequential()\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "model_lstm.compile(loss = 'binary_crossentropy', optimizer = 'adam', metrics = ['accuracy'])\n",
    "model_lstm.fit(X_train_reshaped[:10000], y_train[:10000], batch_size = 32, epochs= 1, verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df8505a9b005e46fc2b3e4310bded0da",
     "grade": false,
     "grade_id": "cell-fcdeb583aed5f12b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "model_lstm.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "140f0131cf508c6c29f8b1f405636372",
     "grade": false,
     "grade_id": "cell-f5333e4963860835",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Q7. Please complete the code below to report the performance of the LSTM model.\n",
    "\n",
    "**Note:** Please name the model predictions on the validation data (``X_valid_reshaped``) as: ``y_pred_lstm``!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "67058e34486aa0f8bd3ff7ed7d61737f",
     "grade": false,
     "grade_id": "cell-d033bf9d65ba1aef",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# YOUR CODE HERE\n",
    "raise NotImplementedError()\n",
    "\n",
    "threshold = 0.5\n",
    "print(classification_report(y_validation, y_pred_lstm>threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "737498b8e44acd6a2cd1edb25f349666",
     "grade": true,
     "grade_id": "cell-7a3ceb071c10868c",
     "locked": true,
     "points": 7,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ddfff79a80b0b6c30e42920ae3393820",
     "grade": false,
     "grade_id": "cell-a84223285ef3344d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Next Steps\n",
    "\n",
    "Well done! You have learnt how to develop CNN and LSTM models on real-world time series datasets. But, what can be done to improve our model?\n",
    "\n",
    "\n",
    "* There are some parameters in the CNN and LSTM models that can be optimized such as:\n",
    "        1. number of filters in the CNN model.\n",
    "        2. number of layers in the LSTM model. \n",
    "\n",
    "\n",
    "* The model architectures such as number of layers can be explored further to improve the performance.\n",
    "\n",
    "\n",
    "* We can develop a new model by combining CNN and LSTM layers.\n",
    "\n",
    "\n",
    "* We can use a completely unseen dataset to test our trained model. For this purpose, two datasets can be used:\n",
    "        1.MIT-BIH Supraventricular Arrhythmia Database (https://physionet.org/content/svdb/1.0.0/)\n",
    "        2. St Petersburg INCART 12-lead Arrhythmia Database (https://physionet.org/content/incartdb/1.0.0/)  \n",
    "        \n",
    "        \n",
    "##### So, if you are interested in learning more about time series classification using ECG signals and doing your final report (and/or perhaps your Master thesis) on this topic, please contact me :)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "da646d13c56e03148eb0a1becee49368",
     "grade": false,
     "grade_id": "cell-c8d3d03a0f06085d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Thank you!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
